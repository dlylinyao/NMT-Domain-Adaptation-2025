{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T11:36:20.960995Z",
     "iopub.status.busy": "2026-01-08T11:36:20.960280Z",
     "iopub.status.idle": "2026-01-08T11:36:34.232253Z",
     "shell.execute_reply": "2026-01-08T11:36:34.231535Z",
     "shell.execute_reply.started": "2026-01-08T11:36:20.960944Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.4/101.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m529.7/529.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "grain 0.2.15 requires protobuf>=5.28.3, but you have protobuf 4.25.8 which is incompatible.\n",
      "ray 2.52.1 requires click!=8.3.*,>=7.0, but you have click 8.3.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mEnvironment setup and imports completed.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q sacrebleu unbabel-comet transformers[torch] datasets evaluate sentencepiece\n",
    "\n",
    "# Standard Library Imports\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "# Data Processing & Math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Hugging Face Libraries\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "\n",
    "# Comet Library\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "print(\"Environment setup and imports completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T10:08:01.461588Z",
     "iopub.status.busy": "2026-01-08T10:08:01.460830Z",
     "iopub.status.idle": "2026-01-08T10:08:11.410917Z",
     "shell.execute_reply": "2026-01-08T10:08:11.410144Z",
     "shell.execute_reply.started": "2026-01-08T10:08:01.461557Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Directory: /kaggle/input/final-data/processed\n",
      "\n",
      "======== Analyzing: UNPC Cleaned (Train) ========\n",
      "1. Line Count (Sentence Pairs): 462483\n",
      "2. Avg Length (Source EN): 19.45 words, 123.11 chars\n",
      "   Avg Length (Target ZH): 2.81 words, 49.63 chars\n",
      "3. Vocabulary Size (Unique Tokens): Source=372300, Target=669711\n",
      "4. Avg Length Ratio (Src/Tgt Char Ratio): 2.42\n",
      "5. Calculating Character Entropy...\n",
      "   Source Entropy: 4.8487\n",
      "   Target Entropy: 8.5990\n",
      "\n",
      "----- Random Sample (Qualitative Check) -----\n",
      "[235377] EN: (d) Human rights and disability;\n",
      "      ZH: (d) äººæƒä¸æ®‹ç–¾é—®é¢˜ï¼›\n",
      "[401839] EN: The Council then proceeded to the vote on the draft resolution before it (S/2000/1124).\n",
      "      ZH: å®‰ç†ä¼šæ¥ç€å°±é¢å‰çš„å†³è®®è‰æ¡ˆï¼ˆS/2000/1124ï¼‰è¿›è¡Œè¡¨å†³ã€‚\n",
      "[422352] EN: B. Organization of UNISPACE III\n",
      "      ZH: B. ç¬¬ä¸‰æ¬¡å¤–ç©ºä¼šè®®çš„å®‰æ’\n",
      "[293831] EN: In May 1999, UNFPA approved its first project for the former Yugoslav Republic of Macedonia, including an initial amount of $5,000 earmarked for technical needs assessment.\n",
      "      ZH: 1999å¹´5æœˆ,äººå£åŸºé‡‘æ ¸å‡†äº†å…¶æ´åŠ©å‰å—æ–¯æ‹‰å¤«çš„é©¬å…¶é¡¿å…±å’Œå›½çš„ç¬¬ä¸€ä¸ªé¡¹ç›®,åŒ…æ‹¬ä¸“é—¨æ‹¨ç»™ç”¨äºæŠ€æœ¯éœ€æ±‚è¯„ä¼°çš„åˆæœŸèµ„é‡‘5 000ç¾å…ƒã€‚\n",
      "[77094] EN: The Working Group met again in May and June 2001 to focus on conflict prevention, post-conflict peace-building and education.\n",
      "      ZH: å·¥ä½œç»„äº2001å¹´5æœˆå’Œ6æœˆå†æ¬¡å¼€ä¼šï¼Œé›†ä¸­è®¨è®ºé¢„é˜²å†²çªã€å†²çªåçš„å’Œå¹³å»ºè®¾å’Œæ•™è‚²é—®é¢˜ã€‚\n",
      "[151171] EN: 8. Mr. Jaremczuk (Poland), Mr. Bliznikas (Lithuania) and Mr. Stanescu (Romania) said that their delegations wished to align themselves with the statement made by the representative of Austria on behalf of the European Union.\n",
      "      ZH: 8. Jaremczukå…ˆç”Ÿ(æ³¢å…°)ã€Bliznikaså…ˆç”Ÿ(ç«‹é™¶å®›)å’ŒStanescuå…ˆç”Ÿ(ç½—é©¬å°¼äºš)è¯´,ä»–ä»¬çš„ä»£è¡¨å¸Œæœ›èµåŒå¥¥åœ°åˆ©ä»£è¡¨,ä»£è¡¨æ¬§æ´²è”ç›Ÿçš„åä¹‰æ‰€ä½œçš„å‘è¨€ã€‚\n",
      "[45023] EN: Cyprus has faced the HIV/AIDS problem since 1986, but remains a low-prevalence country.\n",
      "      ZH: å¡æµ¦è·¯æ–¯è‡ª1986å¹´èµ·é¢ä¸´è‰¾æ»‹ç—…æ¯’/è‰¾æ»‹ç—…é—®é¢˜ï¼Œä½†ä¸€ç›´æ˜¯ä½å‘ç—…å›½å®¶ã€‚\n",
      "[21954] EN: C. Communications received from 20 June 2000 to 15 June 2001 and reports of the Secretary-General\n",
      "      ZH: C. 2000å¹´6æœˆ20æ—¥è‡³2001å¹´6æœˆ15æ—¥æ”¶åˆ°çš„æ¥ æ–‡å’Œç§˜ä¹¦é•¿çš„æŠ¥å‘Š\n",
      "[263373] EN: 175. Institutional infrastructure.\n",
      "      ZH: 175. ä½“åˆ¶åŸºç¡€è®¾æ–½ã€‚\n",
      "[394512] EN: Typically, the well will produce from one zone inside a production &quot; string &quot; or tube inside the casing, and from the other zone oil will be produced in the area between the production string and the casing (known as the &quot; annulus &quot; ).\n",
      "      ZH: æ²¹äº•é€šå¸¸ä»ä¸€ä¸ªåŒºçš„æ²¹å±‚ &quot; å¥—ç®¡ &quot; ï¼Œå³å¥—ç®¡ä¸­çš„ç®¡é“é‡‡æ²¹ï¼Œè€Œå¦ä¸€ä¸ªåŒºåˆ™ä»æ²¹å±‚å¥—ç®¡å’Œé’»äº•å¥—ç®¡ä¹‹é—´çš„åŒºåŸŸ(ç§°ä¸º &quot; ç¯å¸¦ &quot; )é‡‡æ²¹ã€‚\n",
      "[310782] EN: It was so decided.\n",
      "      ZH: 5. ä¼šè®®å†³å®šå¦‚ä¸Šã€‚\n",
      "[66498] EN: Panel discussion: â€œCriteria to test the development friendliness of international investment agreementsâ€\n",
      "      ZH: å°ç»„è®¨è®ºï¼š &quot; ç”¨æ¥æ£€éªŒå›½é™…æŠ•èµ„åå®šåŠ©ç›Šå‘å±•ç¨‹åº¦çš„æ ‡å‡† &quot;\n",
      "[411646] EN: 2.2 At around 4 a.m. on 10 January 1988, Antonio RodrÃ­guez Cottin was stabbed five times in a car lot outside a discotheque in MocejÃ³n, Toledo.\n",
      "      ZH: 2.2. 1988å¹´1æœˆ10æ—¥å¤§çº¦æ—©ä¸Š4ç‚¹ï¼ŒAntonio RodrÃ­guez Cottinåœ¨Toledo Mocejonä¸€è¿ªæ–¯ç§‘èˆå…å¤–çš„åœè½¦å¤„è¢«æ…äº†äº”åˆ€ã€‚\n",
      "[395190] EN: In this respect, Indonesia received the visit of the Special Rapporteur on Torture in 1991, the Special Rapporteur on Summary or Arbitrary Executions in 1994 and, in 1995, the highest authority in the field of human rights, the High Commissioner.\n",
      "      ZH: åœ¨æ­¤æ–¹é¢ï¼Œå°åº¦å°¼è¥¿äºšäº1991å¹´æ¥å¾…äº†é…·åˆ‘é—®é¢˜ç‰¹åˆ«æŠ¥å‘Šå‘˜çš„æ¥è®¿ï¼Œäº1994å¹´æ¥å¾…äº†å³å†³å¤„å†³æˆ–ä»»æ„å¤„å†³é—®é¢˜æŠ¥å‘Šå‘˜çš„æ¥è®¿å¹¶äº1995å¹´æ¥å¾…äº†é«˜çº§ä¸“å‘˜è¿™ä¸€äººæƒé¢†åŸŸä¸­æœ€é«˜æƒå¨çš„æ¥è®¿ã€‚\n",
      "[112584] EN: Sen describes this matching in terms of the Kantian concept of &quot; perfect obligation &quot; .\n",
      "      ZH: é˜¿é©¬è’‚äºšÂ·æ£®ç”¨åº·å¾·çš„ &quot; ç»å¯¹ä¹‰åŠ¡ &quot; æ¥æè¿°è¿™ç§åŒ¹é…ã€‚\n",
      "[27149] EN: Number of refugees assisted to return.\n",
      "      ZH: ä¸ºè¿”å›è€Œå¾—åˆ°ååŠ©çš„éš¾æ°‘äººæ•°ã€‚\n",
      "[304637] EN: It will include, inter alia, representatives of the Department for Disarmament Affairs, the Office for the Coordination of Humanitarian Affairs, UNHCR, UNICEF, UNDP, UNOPS, WFP, FAO, the World Bank and WHO.\n",
      "      ZH: è¯¥ç»„åŒ…æ‹¬ä¸‹åˆ—ç»„ç»‡çš„ä»£è¡¨ï¼šè£å†›äº‹åŠ¡éƒ¨ã€äººé“ä¸»ä¹‰äº‹åŠ¡åè°ƒå…ã€éš¾æ°‘ä¸“å‘˜åŠäº‹å¤„ã€å„¿ç«¥åŸºé‡‘ä¼šã€å¼€å‘è®¡åˆ’ç½²ã€é¡¹ç›®å…ã€ç²®é£Ÿè®¡åˆ’ç½²ã€ç²®å†œç»„ç»‡ã€ä¸–ç•Œé“¶è¡Œå’Œå«ç”Ÿç»„ç»‡ã€‚\n",
      "[415798] EN: In subparagraph (b), in the last sentence, after the words &quot; and sensitizing policy makers &quot; , insert the words &quot; and more importantly, local communities at the grass-roots level &quot; ; and after the words &quot; HIV/AIDS &quot; , replace the word &quot; epidemic &quot; with the words &quot; and other epidemics, such as malaria &quot; .\n",
      "      ZH: åœ¨ï¼ˆbï¼‰åˆ†æ®µæœ€åä¸€å¥ä¸­ï¼Œåœ¨ &quot; ä½¿å†³ç­–è€…äº†è§£ &quot; åæ’å…¥ &quot; ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œä½¿åŸºå±‚çš„åœ°æ–¹ç¤¾åŒºäº†è§£ &quot; ï¼›å¹¶åœ¨ &quot; è‰¾æ»‹ç—…æ¯’/è‰¾æ»‹ç—… &quot; çš„åé¢æ’å…¥ &quot; ä»¥åŠå¦‚è™ç–¾ç­‰å…¶ä»–æµè¡Œç—… &quot; ã€‚\n",
      "[111031] EN: Decision 1 (53) on Australia 18\n",
      "      ZH: å…³äºæ¾³å¤§åˆ©äºšçš„ç¬¬1(53)å·å†³å®š. 19\n",
      "[55919] EN: Yet this is obviously not the case.\n",
      "      ZH: ç„¶è€Œ,äº‹å®æ˜¾ç„¶ä¸æ˜¯è¿™æ ·ã€‚\n",
      "\n",
      "======== Analyzing: TED Cleaned (Train) ========\n",
      "1. Line Count (Sentence Pairs): 50000\n",
      "2. Avg Length (Source EN): 15.77 words, 87.08 chars\n",
      "   Avg Length (Target ZH): 2.35 words, 35.95 chars\n",
      "3. Vocabulary Size (Unique Tokens): Source=65426, Target=109915\n",
      "4. Avg Length Ratio (Src/Tgt Char Ratio): 2.44\n",
      "5. Calculating Character Entropy...\n",
      "   Source Entropy: 4.5101\n",
      "   Target Entropy: 8.8156\n",
      "\n",
      "----- Random Sample (Qualitative Check) -----\n",
      "[32625] EN: Now I'd like to say I woke up one morning and said, \"I'm going to write about collaborative consumption,\" but actually it was a complicated web of seemingly disconnected ideas.\n",
      "      ZH: å…¶å®æˆ‘æ˜¯å¾ˆæƒ³è¿™ä¹ˆè¯´ï¼Œâ€œæœ‰å¤©æ—©ä¸Šï¼Œæˆ‘åˆšé†’å°±è§‰å¾—ï¼Œ å…³äºåä½œæ¶ˆè´¹ï¼Œæˆ‘å¾—å†™ç‚¹ä»€ä¹ˆä¸œè¥¿ã€‚â€ ä¸è¿‡äº‹å®ä¸Šè¿™æƒ³æ³•æ˜¯ä¸€ä¸ªå¤æ‚ç½‘ç»œï¼Œ å®ƒçœ‹ä¼¼ç”±ä¸€äº›é£é©¬ç‰›ä¸ç›¸åŠçš„æƒ³æ³•æ„æˆã€‚\n",
      "[3226] EN: She goes village to village doing screenings.\n",
      "      ZH: å› ä¸ºå¥¹ä¼šå¥”æ³¢äºå¥½å‡ ä¸ªæ‘è½ä¹‹é—´è¿›è¡Œæ£€æµ‹\n",
      "[35242] EN: And you can show, just by looking at data from literature, that vector-borne diseases are more harmful than non-vector-borne diseases.\n",
      "      ZH: è€Œä¸”ï¼Œä»…ä»…é€šè¿‡æŸ¥çœ‹æ–‡çŒ®ä¸­çš„æ•°æ®ï¼Œä½ å°±ä¼šå‘ç° é€šè¿‡è™«åª’ä¼ æ’­çš„ç–¾ç—…æ¯”é‚£äº› é€šè¿‡éè™«åª’ä¼ æ’­çš„ç–¾ç—…æ›´åŠ æœ‰å®³ã€‚\n",
      "[33634] EN: \"Got something on my hair?\" Here we go. Alright.\n",
      "      ZH: â€œè¿˜æ˜¯æˆ‘å¤´å‘æ²¾ä¸Šä»€ä¹ˆä¸œè¥¿äº†?â€ å—¯ï¼Œå°±æ˜¯è¿™ä¸ªã€‚\n",
      "[3797] EN: But anyway, you know, I just think that to be here with all of you accomplished young people -- literally, some of you, the architects building our brighter future.\n",
      "      ZH: å¥½å§ ä¸ç®¡æ€æ · æˆ‘åªæ˜¯è§‰å¾— è·Ÿä½ ä»¬åœ¨ä¸€èµ· è·Ÿä½ ä»¬è¿™äº›å¹´è½»äººåœ¨ä¸€èµ· è€Œä¸”ä½ ä»¬å½“ä¸­æœ‰äº›æ˜¯å»ºç­‘å¸ˆ å°±æ˜¯è¦æ­å»ºæˆ‘ä»¬æ›´ç¾å¥½çš„æœªæ¥çš„\n",
      "[28025] EN: We finally, after 40 years of knowing that wholegrain was a healthier option, we're finally getting to the point where we actually are tipping over and attempting to actually eat them.\n",
      "      ZH: å¦‚ä»Šï¼Œåœ¨æˆ‘ä»¬40å¹´å‰ä¾¿å·²çŸ¥é“ å…¨éº¦é¢åŒ…è¦æ›´åŠ å¥åº·çš„æƒ…å†µä¸‹ï¼Œ ç»ˆäºåˆ°å–å¾—é‡å¤§çªç ´çš„æ—¶å€™äº†ï¼Œ æˆ‘ä»¬ç»ˆäºæœ‰æœºä¼šçœŸæ­£å»åƒå…¨éº¦é¢åŒ…äº†ã€‚\n",
      "[39251] EN: http://www.ted.com/talks/kathryn_schulz_don_t_regret_regret.html\n",
      "      ZH: http://www.ted.com/talks/lang/zh-cn/kathryn_schulz_don_t_regret_regret.html\n",
      "[25688] EN: So I put it through a G.C., a Gas Chromatograph that I have in my office, and itâ€™s about 400.\n",
      "      ZH: äºæ˜¯ æˆ‘ç”¨æˆ‘åŠå…¬å®¤é‡Œçš„æ°”ç›¸è‰²è°±ä»ªåšäº†ä¸€ä¸‹æ£€æµ‹ å‘ç°å¤§çº¦æœ‰400ä¸ª\n",
      "[16921] EN: It has floated away down a dark mythological river whose name begins with an L as far as you can recall, well on your own way to oblivion where you will join those who have forgotten even how to swim and how to ride a bicycle.\n",
      "      ZH: å®ƒå·²æ¼‚èµ°äº†ï¼Œ é¡ºç€ç¥è¯ä¸­çš„é»‘æš—æ²³æµã€‚ ä½ åªèƒ½å›å¿†èµ·å®ƒçš„åå­—ä»¥Lå¼€å§‹ã€‚ å‰©ä¸‹çš„ä½ è®°ä¸èµ·æ¥äº†ã€‚ å°±åœ¨ä½ é€šå¾€é—å¿˜çš„è·¯ä¸Šï¼Œ ä½ å°†åŠ å…¥é‚£äº› ç”šè‡³å¿˜è®°äº†å¦‚ä½•æ¸¸æ³³ å’Œéª‘è‡ªè¡Œè½¦çš„äººä»¬ã€‚\n",
      "[43263] EN: And they were to blow up strategic targets and take over the country, and they were foiled by a Nigerian James Bond called Coyote Williams, and a Jewish Nazi hunter.\n",
      "      ZH: ä»–ä»¬çªç„¶æ‰“å‡»æˆ˜ç•¥ç›®æ ‡ æŒæ¡äº†æ•´ä¸ªå›½å®¶ï¼Œç»“æœè¢«ä¸€ä¸ªäººé˜»æ­¢äº†ï¼Œ è¿™äººæ˜¯ä¸€ä¸ªå°¼æ—¥åˆ©äºšçš„è©¹å§†å£«.é‚¦å¾·ï¼Œåå«è€ƒå°”æ¬§ç‰¹.å¨å»‰å§†æ–¯. å’Œä¸€ä¸ªçŠ¹å¤ªäººâ€”â€”ä¸€ä¸ªçŠ¹å¤ªçš„â€œçº³ç²¹çŒäººâ€ã€‚\n",
      "[19119] EN: But whoever decided that a chicken should look like a heart, a giraffe, a star?\n",
      "      ZH: åˆæ˜¯è°å†³å®šäº†é¸¡åº”è¯¥é•¿ä»€ä¹ˆæ ·ï¼Œ å¿ƒå½¢ï¼Œé•¿é¢ˆé¹¿å½¢ï¼Œæˆ–æ˜¯æ˜Ÿå½¢ï¼Ÿ\n",
      "[19399] EN: And I actually said at one conference a couple of years ago, \"Give me my damn data, because you people can't be trusted to keep it clean.\"\n",
      "      ZH: æˆ‘å®é™…ä¸Šåœ¨å‡ å¹´ä¹‹å‰åœ¨ä¸€ä¸ªä¼šè®®ä¸Šè¯´åˆ°ï¼Œ ç»™æˆ‘è¯¥æ­»çš„æ•°æ®ï¼Œ å› ä¸ºä½ ä»¬è¿™äº›äººä¸èƒ½è¢«ä¿¡ä»»ï¼Œè®©è¿™äº›æ•°æ®æ¸…æ¥šæ˜ç™½ã€‚\n",
      "[43430] EN: With PISA, we wanted to measure how they actually deliver equity, in terms of ensuring that people from different social backgrounds have equal chances.\n",
      "      ZH: æˆ‘ä»¬å°±ç”¨PISAæ¥æµ‹è¯•ä»–ä»¬æ˜¯å¦‚ä½•åšåˆ°æ•™è‚²å…¬å¹³ï¼Œ ä¹Ÿå°±æ˜¯å¦‚ä½•ç¡®ä¿å­¦ç”Ÿ æ— è®ºå…¶æ¥è‡ªå“ªç§ä¸åŒçš„ç¤¾ä¼šèƒŒæ™¯éƒ½æœ‰å‡ç­‰çš„å—æ•™è‚²æœºä¼šã€‚\n",
      "[1198] EN: And we're now going to put some chemistry inside and do some chemistry in this cell.\n",
      "      ZH: æˆ‘ä»¬å‡†å¤‡åœ¨é‡Œé¢æ”¾ä¸Šä¸€äº›åŒ–å­¦ç‰©è´¨ï¼Œå¹¶åœ¨ç»†èƒå†…è¿›è¡Œä¸€äº›åŒ–å­¦ååº”ã€‚\n",
      "[27825] EN: You get swine flu in Mexico, it's a problem for Charles de Gaulle Airport 24 hours later.\n",
      "      ZH: ä½ åœ¨å¢¨è¥¿å“¥æŸ“ä¸ŠçŒªæµæ„Ÿ 24 å°æ—¶åï¼Œå®ƒä¾¿æˆä¸º æˆ´é«˜ä¹æœºåœºçš„é—®é¢˜\n",
      "[37675] EN: I know I've talked about Agnes here before, but I want to give you an update on Agnes.\n",
      "      ZH: æˆ‘çŸ¥é“æˆ‘ä»¥å‰åœ¨è¿™é‡Œå·²ç»è¯´è¿‡å…³äºè‰¾æ ¼å°¼ä¸çš„äº‹æƒ…äº† ä½†ç°åœ¨æˆ‘è¦å‘Šè¯‰ä½ ä»¬ä¸€äº›å¥¹çš„æœ€æ–°æ¶ˆæ¯\n",
      "[48084] EN: And more importantly, that line between good and evil -- which privileged people like to think is fixed and impermeable, with them on the good side, and the others on the bad side -- I knew that line was movable, and it was permeable.\n",
      "      ZH: æ›´é‡è¦çš„æ˜¯ï¼Œå–„æ¶ä¹‹é—´çš„ç•Œé™â€”â€” ç‰¹æƒé˜¶å±‚å–œæ¬¢è®¤å®šè¿™ä¸ªç•Œé™æ˜¯å›ºå®šä¸”ä¸å¯é€¾è¶Šçš„ï¼Œ è®¤ä¸ºä»–ä»¬æ˜¯åœ¨å–„çš„ä¸€è¾¹ï¼Œå…¶ä»–äººåœ¨æ¶çš„ä¸€è¾¹â€”â€” è€Œæˆ‘ä»¥å‰å°±çŸ¥é“è¿™ä¸ªç•Œé™æ˜¯å¯ä»¥ç§»åŠ¨çš„ï¼Œè€Œä¸”æ˜¯å¯é€¾è¶Šçš„ã€‚\n",
      "[15910] EN: That's in order to delay -- until act five, he can kill him.\n",
      "      ZH: è¿™ä¸€åˆ‡éƒ½æ˜¯ä¸ºäº†æ¨è¿Ÿâ€”â€”ç›´åˆ°åœ¨ç¬¬äº”å¹•ä»–æ€æ‰ä»–å”å”ã€‚\n",
      "[21942] EN: In 2005, remittances -- I just took one country, Nigeria skyrocketing -- skyrocketing is too dramatic, but increasing dramatically.\n",
      "      ZH: 2005å¹´ï¼Œ æ±‡æ¬¾--æ‹¿å°¼æ—¥åˆ©äºšä½œä¾‹å­-- ä½ ä»¬çŸ¥é“ï¼Œé‚£æ˜¯é£é€Ÿä¸Šå‡-è¯´é£é€Ÿä¸Šå‡é‚£å°±å‘å±•å¾—å¤ªå¿«äº† ä½†çš„ç¡®å¢é•¿åœ°éå¸¸å¿«\n",
      "[29804] EN: June Cohen: Frank, that was beautiful, so touching.\n",
      "      ZH: ç¼Â·ç§‘æ©:å¼—å…°å…‹,é‚£å¤ªç¾å¦™äº† çœŸçš„å¾ˆæ„Ÿäºº\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import math\n",
    "# import random\n",
    "# from collections import Counter\n",
    "\n",
    "POSSIBLE_PATHS = [\n",
    "    \"/kaggle/input/final-data/processed\", \n",
    "    \"/kaggle/input/final-data\", \n",
    "    \"/kaggle/input/processed\"\n",
    "]\n",
    "DATA_DIR = None\n",
    "for p in POSSIBLE_PATHS:\n",
    "    if os.path.exists(p) and \"train_unpc.en\" in os.listdir(p):\n",
    "        DATA_DIR = p\n",
    "        break\n",
    "\n",
    "if not DATA_DIR:\n",
    "    print(\"Data path not found. Please manually modify the DATA_DIR variable!\")\n",
    "else:\n",
    "    print(f\"Data Directory: {DATA_DIR}\")\n",
    "\n",
    "def analyze_corpus(name, src_filename, tgt_filename):\n",
    "    src_path = os.path.join(DATA_DIR, src_filename)\n",
    "    tgt_path = os.path.join(DATA_DIR, tgt_filename)\n",
    "    \n",
    "    if not os.path.exists(src_path) or not os.path.exists(tgt_path):\n",
    "        print(f\"âš ï¸ Skipping {name}: File not found\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n======== Analyzing: {name} ========\")\n",
    "    \n",
    "    # Statistical variables\n",
    "    src_lines, tgt_lines = [], []\n",
    "    src_lens_char, tgt_lens_char = [], []\n",
    "    src_lens_word, tgt_lens_word = [], []\n",
    "    src_vocab, tgt_vocab = set(), set()\n",
    "    \n",
    "    # Read Source\n",
    "    with open(src_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            src_lines.append(l)\n",
    "            src_lens_char.append(len(l))\n",
    "            words = l.split() # Simple whitespace tokenization\n",
    "            src_lens_word.append(len(words))\n",
    "            src_vocab.update(words)\n",
    "\n",
    "    # Read Target\n",
    "    with open(tgt_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            tgt_lines.append(l)\n",
    "            tgt_lens_char.append(len(l))\n",
    "            words = l.split()\n",
    "            tgt_lens_word.append(len(words))\n",
    "            tgt_vocab.update(words)\n",
    "\n",
    "    count = len(src_lines)\n",
    "    print(f\"1. Line Count (Sentence Pairs): {count}\")\n",
    "    \n",
    "    avg_src_w = sum(src_lens_word) / count\n",
    "    avg_tgt_w = sum(tgt_lens_word) / count\n",
    "    avg_src_c = sum(src_lens_char) / count\n",
    "    avg_tgt_c = sum(tgt_lens_char) / count\n",
    "    \n",
    "    print(f\"2. Avg Length (Source EN): {avg_src_w:.2f} words, {avg_src_c:.2f} chars\")\n",
    "    print(f\"   Avg Length (Target ZH): {avg_tgt_w:.2f} words, {avg_tgt_c:.2f} chars\")\n",
    "\n",
    "    print(f\"3. Vocabulary Size (Unique Tokens): Source={len(src_vocab)}, Target={len(tgt_vocab)}\")\n",
    "\n",
    "    ratios = []\n",
    "    for sl, tl in zip(src_lens_char, tgt_lens_char):\n",
    "        if tl == 0: continue\n",
    "        ratios.append(sl / tl)\n",
    "    if ratios:\n",
    "        avg_ratio = sum(ratios) / len(ratios)\n",
    "        print(f\"4. Avg Length Ratio (Src/Tgt Char Ratio): {avg_ratio:.2f}\")\n",
    "\n",
    "    def get_entropy(text_list):\n",
    "        full_text = \"\".join(text_list)\n",
    "        if not full_text: return 0\n",
    "        counts = Counter(full_text)\n",
    "        total = len(full_text)\n",
    "        ent = 0\n",
    "        for cnt in counts.values():\n",
    "            p = cnt / total\n",
    "            ent -= p * math.log2(p)\n",
    "        return ent\n",
    "\n",
    "    print(\"5. Calculating Character Entropy...\")\n",
    "    print(f\"   Source Entropy: {get_entropy(src_lines):.4f}\")\n",
    "    print(f\"   Target Entropy: {get_entropy(tgt_lines):.4f}\")\n",
    "\n",
    "    print(\"\\n----- Random Sample (Qualitative Check) -----\")\n",
    "    indices = random.sample(range(count), min(20, count))\n",
    "    for idx in indices:\n",
    "        print(f\"[{idx}] EN: {src_lines[idx]}\")\n",
    "        print(f\"      ZH: {tgt_lines[idx]}\")\n",
    "\n",
    "if DATA_DIR:\n",
    "    # Analyze UNPC Cleaned\n",
    "    analyze_corpus(\"UNPC Cleaned (Train)\", \"train_unpc.en\", \"train_unpc.zh\")\n",
    "    \n",
    "    # Analyze TED Cleaned\n",
    "    analyze_corpus(\"TED Cleaned (Train)\", \"train_ted.en\", \"train_ted.zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T10:08:11.412522Z",
     "iopub.status.busy": "2026-01-08T10:08:11.412258Z",
     "iopub.status.idle": "2026-01-08T10:08:17.647005Z",
     "shell.execute_reply": "2026-01-08T10:08:17.646247Z",
     "shell.execute_reply.started": "2026-01-08T10:08:11.412499Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hGPU Available: True\n",
      "Current Device: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q transformers[torch] datasets sacrebleu evaluate sentencepiece\n",
    "\n",
    "#import torch\n",
    "\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Warning: You are using CPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-08T10:08:21.649418Z",
     "iopub.status.busy": "2026-01-08T10:08:21.648756Z",
     "iopub.status.idle": "2026-01-08T10:08:24.677122Z",
     "shell.execute_reply": "2026-01-08T10:08:24.676336Z",
     "shell.execute_reply.started": "2026-01-08T10:08:21.649372Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path confirmed: /kaggle/input/final-data/processed\n",
      "Loading model vocabulary: Helsinki-NLP/opus-mt-en-mul...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c32b5568754ec281c8c681b22eed74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e652a548db374a9e819ce19d4afa3b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d010482eff48ac8a91c2b9d71eddf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/790k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231caa2b0e734763aa0e9d700c9196df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb7786d948f425eb5164d0a68af378f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Analyzing: English Train Set (train_unpc.en)\n",
      "  [Sample 1] Text: a 1999 data are provisional....\n",
      "  [Sample 1] Tokens: ['â–a', 'â–1999', 'â–data', 'â–are', 'â–provisional', '.']\n",
      "  [Sample 2] Text: Recalling its resolution 49/251 of 20 July 1995 on...\n",
      "  [Sample 2] Tokens: ['â–Recalling', 'â–its', 'â–resolution', 'â–49', '/25', '1', 'â–of', 'â–20', 'â–July', 'â–1995', 'â–on', 'â–the', 'â–financing', 'â–of', 'â–the', 'â–Tribunal', 'â–and', 'â–its', 'â–subsequent', 'â–resolutions', 'â–thereon', ',', 'â–the', 'â–latest', 'â–of', 'â–which', 'â–was', 'â–resolution', 'â–53', '/21', '3', 'â–of', 'â–18', 'â–December', 'â–1998,']\n",
      " English Train Set Average Length: 27.84 tokens\n",
      "\n",
      " Analyzing: Chinese Train Set (train_unpc.zh)\n",
      "  [Sample 1] Text: a 1999å¹´æ•°æ®ä¸ºæš‚å®šæ•°æ®ã€‚...\n",
      "  [Sample 1] Tokens: ['â–a', 'â–1999', 'å¹´', 'æ•°', 'æ®', 'ä¸º', 'æš‚', 'å®š', 'æ•°', 'æ®', 'ã€‚']\n",
      "  [Sample 2] Text: å›é¡¾å…¶1995å¹´7æœˆ20æ—¥å…³äºè¯¥æ³•åº­ç»è´¹ç­¹æªçš„ç¬¬49/251å·å†³è®®åŠå…¶åå„é¡¹æœ‰å…³å†³è®®,æœ€è¿‘çš„ä¸€é¡¹æ˜¯1...\n",
      "  [Sample 2] Tokens: ['â–', 'å›', 'é¡¾', 'å…¶', '1995', 'å¹´', '7', 'æœˆ', '20', 'æ—¥', 'å…³', 'äº', 'è¯¥', 'æ³•', 'åº­', 'ç»', 'è´¹', 'ç­¹æª', 'çš„', 'ç¬¬', '49', '/25', '1', 'å·', 'å†³è®®', 'åŠ', 'å…¶', 'å', 'å„', 'é¡¹', 'æœ‰', 'å…³', 'å†³è®®', ',', 'æœ€è¿‘', 'çš„', 'ä¸€', 'é¡¹', 'æ˜¯', '1998', 'å¹´', '12', 'æœˆ', '18', 'æ—¥', 'ç¬¬', '53', '/21', '3', 'å·', 'å†³è®®', ',']\n",
      " Chinese Train Set Average Length: 38.84 tokens\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer Analysis & Data Path Verification\n",
    "#import os\n",
    "#import numpy as np\n",
    "#from transformers import AutoTokenizer\n",
    "\n",
    "POSSIBLE_PATHS = [\n",
    "    \"/kaggle/input/final-data/processed\", \n",
    "    \"/kaggle/input/final-data\",          \n",
    "    \"/kaggle/input/processed\"            \n",
    "]\n",
    "\n",
    "DATA_DIR = None\n",
    "for p in POSSIBLE_PATHS:\n",
    "    if os.path.exists(p) and \"train_unpc.en\" in os.listdir(p):\n",
    "        DATA_DIR = p\n",
    "        break\n",
    "\n",
    "if not DATA_DIR:\n",
    "    print(\"Data not found! Please check the Data panel on the right, copy the 'processed' folder path, and update the paths above.\")\n",
    "else:\n",
    "    print(f\"Data path confirmed: {DATA_DIR}\")\n",
    "\n",
    "MODEL_CHECKPOINT = \"Helsinki-NLP/opus-mt-en-mul\"\n",
    "\n",
    "# Load Tokenizer\n",
    "print(f\"Loading model vocabulary: {MODEL_CHECKPOINT}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "def analyze_file(filename, label):\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    print(f\"\\n Analyzing: {label} ({filename})\")\n",
    "    \n",
    "    lengths = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 10000: break \n",
    "            tokens = tokenizer.tokenize(line.strip())\n",
    "            lengths.append(len(tokens))\n",
    "            \n",
    "            if i < 2:\n",
    "                print(f\"  [Sample {i+1}] Text: {line.strip()[:50]}...\")\n",
    "                print(f\"  [Sample {i+1}] Tokens: {tokens}\")\n",
    "\n",
    "    print(f\" {label} Average Length: {np.mean(lengths):.2f} tokens\")\n",
    "\n",
    "# Run Analysis\n",
    "if DATA_DIR:\n",
    "    analyze_file(\"train_unpc.en\", \"English Train Set\")\n",
    "    analyze_file(\"train_unpc.zh\", \"Chinese Train Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T10:30:55.967119Z",
     "iopub.status.busy": "2026-01-08T10:30:55.966823Z",
     "iopub.status.idle": "2026-01-08T11:01:58.891142Z",
     "shell.execute_reply": "2026-01-08T11:01:58.890411Z",
     "shell.execute_reply.started": "2026-01-08T10:30:55.967090Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef03524e7b55494f9ef442c65b1500ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pre-trained UNPC Model from: /kaggle/input/final-unpc-model ...\n",
      "Successfully loaded UNPC model weights!\n",
      "Loading TED data...\n",
      "TED Train size: 50000\n",
      "Preprocessing TED data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6887612642934f2ab77f8a50fe34e1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700d530240514329a883180484069311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/1163181332.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TED Domain Adaptation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4689' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4689/4689 28:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.547800</td>\n",
       "      <td>2.380073</td>\n",
       "      <td>18.333715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.368200</td>\n",
       "      <td>2.317173</td>\n",
       "      <td>18.197097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.279800</td>\n",
       "      <td>2.298169</td>\n",
       "      <td>17.701789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TED Training Complete! Model saved to /kaggle/working/final_ted_model\n",
      "Backup created: ted_model_backup.zip\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"/kaggle/input/final-data/processed\" \n",
    "\n",
    "CHECKPOINT_PATH = \"/kaggle/input/final-unpc-model\" \n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/final_ted_model\"\n",
    "\n",
    "def load_dataset_from_text(src_path, tgt_path):\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as fs, open(tgt_path, \"r\", encoding=\"utf-8\") as ft:\n",
    "        return Dataset.from_dict({\"source\": [l.strip() for l in fs], \"target\": [l.strip() for l in ft]})\n",
    "\n",
    "SOURCE_PREFIX = \">>zho<< \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [SOURCE_PREFIX + ex for ex in examples[\"source\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"target\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple): preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "print(f\"Loading Pre-trained UNPC Model from: {CHECKPOINT_PATH} ...\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_PATH)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT_PATH)\n",
    "    print(\"Successfully loaded UNPC model weights!\")\n",
    "except OSError as e:\n",
    "    print(\"Error loading model. Please check the path in Input panel.\")\n",
    "    raise e\n",
    "\n",
    "# Load TED data\n",
    "print(\"Loading TED data...\")\n",
    "train_ted = load_dataset_from_text(\n",
    "    os.path.join(DATA_DIR, \"train_ted.en\"), \n",
    "    os.path.join(DATA_DIR, \"train_ted.zh\")\n",
    ")\n",
    "dev_ted = load_dataset_from_text(\n",
    "    os.path.join(DATA_DIR, \"dev_ted.en\"), \n",
    "    os.path.join(DATA_DIR, \"dev_ted.zh\")\n",
    ")\n",
    "print(f\"TED Train size: {len(train_ted)}\")\n",
    "\n",
    "print(\"Preprocessing TED data...\")\n",
    "tokenized_train_ted = train_ted.map(preprocess_function, batched=True)\n",
    "tokenized_dev_ted = dev_ted.map(preprocess_function, batched=True)\n",
    "\n",
    "# Set training arguments (TED Domain Adaptation)\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",      \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,        \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,         \n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    logging_steps=50,        \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train_ted,\n",
    "    eval_dataset=tokenized_dev_ted,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting TED Domain Adaptation...\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "print(f\"TED Training Complete! Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "shutil.make_archive(\"/kaggle/working/ted_model_backup\", 'zip', OUTPUT_DIR)\n",
    "print(\"Backup created: ted_model_backup.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T11:37:14.948898Z",
     "iopub.status.busy": "2026-01-08T11:37:14.948322Z",
     "iopub.status.idle": "2026-01-08T11:50:58.226275Z",
     "shell.execute_reply": "2026-01-08T11:50:58.225482Z",
     "shell.execute_reply.started": "2026-01-08T11:37:14.948861Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading COMET model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfed0429fab74e06afba8f4b49bd865d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.3.5 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../root/.cache/huggingface/hub/models--Unbabel--wmt20-comet-da/snapshots/87819f4d6d4f17e0d1752cc9e0ccfa2064997219/checkpoints/model.ckpt`\n",
      "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/core/saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline on TED Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39c3c9a6f5d4877a08878f4bb2abb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/310M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35454593600c46818f76caccc5c9233f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24dff552f1841ed967eca7123fef89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/310M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:20<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for Baseline:\n",
      "   Src: So Europe is not just an example now to emulate; it's an enemy to fight and to resist.\n",
      "   Ref: å› æ­¤æ¬§æ´²ä»ä¹‹å‰çš„æ¦œæ ·å˜æˆäº†å½“æ—¶çš„ç«äº‰è€… å˜æˆäº†éœ€è¦åæŠ—å’Œæ–—äº‰çš„æ•Œäºº\n",
      "   Hyp: æ‰€ä»¥æ¬§æ´²ä¸æ˜¯ç°åœ¨çš„ä¸€ä¸ªä¾‹å­,æ˜¯æˆ˜æ–—å’ŒåæŠ—çš„æ•Œäººã€‚\n",
      "--------------------\n",
      "   Src: I do want to talk -- the absolute icing on this cemetery cake is the Barricini family mausoleum nearby.\n",
      "   Ref: ä½†æˆ‘æƒ³è¯´ -- å…¬å¢“æ‰€ç”¨è›‹ç³•çš„ç³–è¡£éƒ¨åˆ† æ˜¯åœ¨Barricini å®¶æ—çš„é™µå¢“é™„è¿‘.\n",
      "   Hyp: æˆ‘è¦è°ˆè°ˆ -- å¯¹è¿™åœºè‘¬ç¤¼è›‹ç³•çš„ç»å¯¹å‘å°„æ˜¯å·´é‡Œè¥¿å°¼å®¶å±çš„å¤§éº»\n",
      "--------------------\n",
      "   Src: It's a parabolic Scheffler solar cooker.\n",
      "   Ref: è¿™æ˜¯ä¸€ä¸ªæŠ›ç‰©å½¢çš„å…¨è‡ªåŠ¨å¤ªé˜³èƒ½ç‚Šå…·ã€‚\n",
      "   Hyp: é€™æ˜¯å€‹å¤ªé˜³çš„å°å­\n",
      "--------------------\n",
      "Reading Tatoeba from: /kaggle/input/tatoeba-test/tatoeba-test-v2023-09-26.eng-zho.txt\n",
      "Loaded 5526 sentences from Tatoeba (Simplified Chinese only).\n",
      "Evaluating Baseline on Tatoeba...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "INFO:pytorch_lightning.utilities.rank_zero:ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 691/691 [01:07<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for Baseline:\n",
      "   Src: Tom got drunk with his friends in the park.\n",
      "   Ref: æ±¤å§†å’Œä»–çš„æœ‹å‹ä»¬åœ¨å…¬å›­å–é†‰äº†\n",
      "   Hyp: æ‰˜å§†å’Œä»–åœ¨å…¬å›­é‡Œçš„æœ‹å‹å–é†‰äº†\n",
      "--------------------\n",
      "   Src: Tom isn't going to give me that.\n",
      "   Ref: æ±¤å§†ä¸ä¼šåŒæ„çš„\n",
      "   Hyp: æ‰˜å§†ä¸ä¼šç»™æˆ‘é‚£ä¸ª\n",
      "--------------------\n",
      "   Src: 2016 was the hottest year on record.\n",
      "   Ref: 2016å¹´æ˜¯æœ‰è®°å½•ä»¥æ¥æœ€çƒ­çš„ä¸€å¹´ã€‚\n",
      "   Hyp: 2016å¹´æ˜¯è®°å½•ä¸Šæœ€çƒ­çš„å¹´ã€‚\n",
      "--------------------\n",
      "Evaluating Model A (UNPC) on TED Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "INFO:pytorch_lightning.utilities.rank_zero:ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:22<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for Model A (UNPC):\n",
      "   Src: So Europe is not just an example now to emulate; it's an enemy to fight and to resist.\n",
      "   Ref: å› æ­¤æ¬§æ´²ä»ä¹‹å‰çš„æ¦œæ ·å˜æˆäº†å½“æ—¶çš„ç«äº‰è€… å˜æˆäº†éœ€è¦åæŠ—å’Œæ–—äº‰çš„æ•Œäºº\n",
      "   Hyp: å› æ­¤,æ¬§æ´²ç°åœ¨ä¸ä»…æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„ä¾‹å­ã€‚ æˆ˜æ–—å’ŒæŠµæŠ—æ˜¯ä¸€ä¸ªæ•Œäººã€‚\n",
      "--------------------\n",
      "   Src: I do want to talk -- the absolute icing on this cemetery cake is the Barricini family mausoleum nearby.\n",
      "   Ref: ä½†æˆ‘æƒ³è¯´ -- å…¬å¢“æ‰€ç”¨è›‹ç³•çš„ç³–è¡£éƒ¨åˆ† æ˜¯åœ¨Barricini å®¶æ—çš„é™µå¢“é™„è¿‘.\n",
      "   Hyp: æˆ‘æƒ³è°ˆè¯ -- -- è¿™ä¸ªå¢“åœ°è›‹ç³•çš„ç»å¯¹æ··ä¹±æ˜¯é™„è¿‘çš„Barriciniå®¶åº­æ¯›è±¡ã€‚\n",
      "--------------------\n",
      "   Src: It's a parabolic Scheffler solar cooker.\n",
      "   Ref: è¿™æ˜¯ä¸€ä¸ªæŠ›ç‰©å½¢çš„å…¨è‡ªåŠ¨å¤ªé˜³èƒ½ç‚Šå…·ã€‚\n",
      "   Hyp: è¿™æ˜¯ä¸€åå¤ªé˜³æ€§å¨å¸ˆã€‚\n",
      "--------------------\n",
      "Reading Tatoeba from: /kaggle/input/tatoeba-test/tatoeba-test-v2023-09-26.eng-zho.txt\n",
      "Loaded 5526 sentences from Tatoeba (Simplified Chinese only).\n",
      "Evaluating Model A (UNPC) on Tatoeba...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "INFO:pytorch_lightning.utilities.rank_zero:ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 691/691 [01:10<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for Model A (UNPC):\n",
      "   Src: Tom got drunk with his friends in the park.\n",
      "   Ref: æ±¤å§†å’Œä»–çš„æœ‹å‹ä»¬åœ¨å…¬å›­å–é†‰äº†\n",
      "   Hyp: Tomä¸ä»–åœ¨å…¬å›­çš„æœ‹å‹å–é†‰äº†ã€‚\n",
      "--------------------\n",
      "   Src: Tom isn't going to give me that.\n",
      "   Ref: æ±¤å§†ä¸ä¼šåŒæ„çš„\n",
      "   Hyp: æ‰˜å§†ä¸ä¼šç»™æˆ‘è¿™æ ·ã€‚\n",
      "--------------------\n",
      "   Src: 2016 was the hottest year on record.\n",
      "   Ref: 2016å¹´æ˜¯æœ‰è®°å½•ä»¥æ¥æœ€çƒ­çš„ä¸€å¹´ã€‚\n",
      "   Hyp: 2016å¹´æ˜¯è®°å½•æœ€çƒ­çš„å¹´ä»½ã€‚\n",
      "--------------------\n",
      "Evaluating Model B (TED) on TED Test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "INFO:pytorch_lightning.utilities.rank_zero:ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:23<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for Model B (TED):\n",
      "   Src: So Europe is not just an example now to emulate; it's an enemy to fight and to resist.\n",
      "   Ref: å› æ­¤æ¬§æ´²ä»ä¹‹å‰çš„æ¦œæ ·å˜æˆäº†å½“æ—¶çš„ç«äº‰è€… å˜æˆäº†éœ€è¦åæŠ—å’Œæ–—äº‰çš„æ•Œäºº\n",
      "   Hyp: æ‰€ä»¥æ¬§æ´²ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„ä¾‹å­, è€Œæ˜¯ä¸€ç§æˆ˜æ–—å’ŒæŠµæŠ—çš„æ•Œäººã€‚\n",
      "--------------------\n",
      "   Src: I do want to talk -- the absolute icing on this cemetery cake is the Barricini family mausoleum nearby.\n",
      "   Ref: ä½†æˆ‘æƒ³è¯´ -- å…¬å¢“æ‰€ç”¨è›‹ç³•çš„ç³–è¡£éƒ¨åˆ† æ˜¯åœ¨Barricini å®¶æ—çš„é™µå¢“é™„è¿‘.\n",
      "   Hyp: æˆ‘æƒ³è¯´è¯â€”â€” è¿™ä¸ªå¢“åœ°è›‹ç³•ä¸Šçš„ç»å¯¹å…´å¥‹ æ˜¯å·´é‡Œè¥¿å°¼çš„å®¶å±æ¯›ç´¢é‡Œå§†é™„è¿‘ã€‚\n",
      "--------------------\n",
      "   Src: It's a parabolic Scheffler solar cooker.\n",
      "   Ref: è¿™æ˜¯ä¸€ä¸ªæŠ›ç‰©å½¢çš„å…¨è‡ªåŠ¨å¤ªé˜³èƒ½ç‚Šå…·ã€‚\n",
      "   Hyp: å®ƒæ˜¯ä¸€åæ¨¡æ‹Ÿæ€§çš„Schefflerå¤ªé˜³èƒ½å¨å¸ˆã€‚\n",
      "--------------------\n",
      "Reading Tatoeba from: /kaggle/input/tatoeba-test/tatoeba-test-v2023-09-26.eng-zho.txt\n",
      "Loaded 5526 sentences from Tatoeba (Simplified Chinese only).\n",
      "Evaluating Model B (TED) on Tatoeba...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "INFO:pytorch_lightning.utilities.rank_zero:ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 691/691 [01:16<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for Model B (TED):\n",
      "   Src: Tom got drunk with his friends in the park.\n",
      "   Ref: æ±¤å§†å’Œä»–çš„æœ‹å‹ä»¬åœ¨å…¬å›­å–é†‰äº†\n",
      "   Hyp: æ±¤å§†å’Œä»–åœ¨å…¬å›­é‡Œçš„æœ‹å‹å–é†‰äº†ã€‚\n",
      "--------------------\n",
      "   Src: Tom isn't going to give me that.\n",
      "   Ref: æ±¤å§†ä¸ä¼šåŒæ„çš„\n",
      "   Hyp: (ç¬‘å£°) æ±¤å§†ä¸ä¼šç»™æˆ‘è¿™ä»¶äº‹ã€‚\n",
      "--------------------\n",
      "   Src: 2016 was the hottest year on record.\n",
      "   Ref: 2016å¹´æ˜¯æœ‰è®°å½•ä»¥æ¥æœ€çƒ­çš„ä¸€å¹´ã€‚\n",
      "   Hyp: 2016å¹´æ˜¯è®°å½•ä¸Šæœ€çƒ­çš„ä¸€å¹´ã€‚\n",
      "--------------------\n",
      "FINAL RESULTS TABLE:\n",
      "| Model          | Test Set   |   BLEU |   chrF |   COMET |\n",
      "|:---------------|:-----------|-------:|-------:|--------:|\n",
      "| Baseline       | TED Test   |  14.19 |  17.83 | -0.3222 |\n",
      "| Baseline       | Tatoeba    |  20.87 |  18.57 |  0.2637 |\n",
      "| Model A (UNPC) | TED Test   |  16.07 |  18.92 | -0.1693 |\n",
      "| Model A (UNPC) | Tatoeba    |  19.48 |  18.99 |  0.3034 |\n",
      "| Model B (TED)  | TED Test   |  21.01 |  24.72 |  0.0018 |\n",
      "| Model B (TED)  | Tatoeba    |  21.07 |  21.83 |  0.2977 |\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#import torch\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#from datasets import Dataset\n",
    "#from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "#import evaluate\n",
    "#from comet import download_model, load_from_checkpoint\n",
    "\n",
    "\n",
    "DATA_DIR = \"/kaggle/input/final-data/processed\"\n",
    "TATOEBA_FILE_PATH = \"/kaggle/input/tatoeba-test/tatoeba-test-v2023-09-26.eng-zho.txt\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MODELS = {\n",
    "    \"Baseline\": \"Helsinki-NLP/opus-mt-en-mul\",\n",
    "    \"Model A (UNPC)\": \"/kaggle/input/final-unpc-model\", \n",
    "    \"Model B (TED)\":  \"/kaggle/input/trained-ted-model\"   \n",
    "}\n",
    "\n",
    "TEST_SETS = {\n",
    "    \"TED Test\": (\"pair\", (\"test_ted.en\", \"test_ted.zh\")),\n",
    "    \"Tatoeba\":  (\"single\", TATOEBA_FILE_PATH) \n",
    "}\n",
    "\n",
    "# Initialize Metrics\n",
    "# BLEU & chrF \n",
    "metric_bleu = evaluate.load(\"sacrebleu\")\n",
    "metric_chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "# COMET \n",
    "print(\"Downloading COMET model...\")\n",
    "try:\n",
    "    comet_model_path = download_model(\"Unbabel/wmt20-comet-da\")\n",
    "    comet_model = load_from_checkpoint(comet_model_path)\n",
    "    use_comet = True\n",
    "except Exception as e:\n",
    "    print(f\"COMET load failed: {e}. Skipping COMET.\")\n",
    "    use_comet = False\n",
    "\n",
    "\n",
    "# Load standard pair files (TED/UNPC)\n",
    "def load_pair_data(src_file, tgt_file):\n",
    "    with open(os.path.join(DATA_DIR, src_file), 'r') as f: src = [l.strip() for l in f]\n",
    "    with open(os.path.join(DATA_DIR, tgt_file), 'r') as f: tgt = [l.strip() for l in f]\n",
    "    return src, tgt\n",
    "\n",
    "# Load Tatoeba single file\n",
    "def load_tatoeba_file(file_path):\n",
    "    print(f\"Reading Tatoeba from: {file_path}\")\n",
    "    srcs = []\n",
    "    tgts = []\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return [], []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            \n",
    "            if len(parts) >= 4 and 'Hans' in parts[1]:\n",
    "                srcs.append(parts[2])\n",
    "                tgts.append(parts[3])\n",
    "                \n",
    "    print(f\"Loaded {len(srcs)} sentences from Tatoeba (Simplified Chinese only).\")\n",
    "    return srcs, tgts\n",
    "\n",
    "# Core Evaluation Function\n",
    "def evaluate_model(model_path, model_name, test_name, src_text, tgt_text):\n",
    "    print(f\"Evaluating {model_name} on {test_name}...\")\n",
    "    \n",
    "    # Load model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(DEVICE)\n",
    "    \n",
    "    # Generate Translations\n",
    "    translations = []\n",
    "    batch_size = 16 \n",
    "    prefix = \">>zho<< \"\n",
    "    \n",
    "    for i in range(0, len(src_text), batch_size):\n",
    "        batch = [prefix + s for s in src_text[i:i+batch_size]]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(**inputs, max_length=128)\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "        translations.extend(decoded)\n",
    "    \n",
    "    # Calculate BLEU (tokenized)\n",
    "    bleu_score = metric_bleu.compute(predictions=translations, references=tgt_text, tokenize='zh')['score']\n",
    "    \n",
    "    # Calculate chrF (character level)\n",
    "    chrf_score = metric_chrf.compute(predictions=translations, references=tgt_text)['score']\n",
    "    \n",
    "    # Calculate COMET\n",
    "    comet_score = 0\n",
    "    if use_comet:\n",
    "        comet_data = [{\"src\": s, \"mt\": m, \"ref\": r} for s, m, r in zip(src_text, translations, tgt_text)]\n",
    "\n",
    "        try:\n",
    "            comet_out = comet_model.predict(comet_data, batch_size=8, gpus=1)\n",
    "            comet_score = comet_out.system_score \n",
    "        except Exception as e:\n",
    "            print(f\"COMET calculation error: {e}\")\n",
    "    \n",
    "    # Print Examples\n",
    "    print(f\"Examples for {model_name}:\")\n",
    "    for i in range(min(3, len(src_text))): \n",
    "        print(f\"   Src: {src_text[i]}\")\n",
    "        print(f\"   Ref: {tgt_text[i]}\")\n",
    "        print(f\"   Hyp: {translations[i]}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    return {\n",
    "        \"BLEU\": round(bleu_score, 2),\n",
    "        \"chrF\": round(chrf_score, 2),\n",
    "        \"COMET\": round(comet_score, 4)\n",
    "    }\n",
    "\n",
    "# Main Loop\n",
    "final_results = []\n",
    "\n",
    "for model_name, model_path in MODELS.items():\n",
    "    for test_name, (load_type, file_info) in TEST_SETS.items():\n",
    "        \n",
    "        if load_type == \"pair\":\n",
    "            src, tgt = load_pair_data(file_info[0], file_info[1])\n",
    "        elif load_type == \"single\":\n",
    "            src, tgt = load_tatoeba_file(file_info)\n",
    "        \n",
    "        if len(src) > 0:\n",
    "            scores = evaluate_model(model_path, model_name, test_name, src, tgt)\n",
    "            \n",
    "            res = {\"Model\": model_name, \"Test Set\": test_name}\n",
    "            res.update(scores)\n",
    "            final_results.append(res)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "df = pd.DataFrame(final_results)\n",
    "print(\"FINAL RESULTS TABLE:\")\n",
    "print(df.to_markdown(index=False))\n",
    "\n",
    "df.to_csv(\"evaluation_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9206452,
     "sourceId": 14414569,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9218849,
     "sourceId": 14433029,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9219223,
     "sourceId": 14433570,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9219489,
     "sourceId": 14433942,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
